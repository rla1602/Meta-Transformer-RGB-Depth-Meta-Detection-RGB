import os, cv2, torch
import numpy as np
from torch.utils.data import Dataset

class MultiModalDataset(Dataset):
    def __init__(self, root, split='train', modalities=['rgb','ir'], img_size=(640,640), transforms=None):
        self.root = root
        self.split = split
        self.modalities = modalities
        self.img_size = img_size
        self.transforms = transforms

        rgb_path = os.path.join(root, 'images', split, 'rgb')
        self.ids = [os.path.splitext(f)[0] for f in os.listdir(rgb_path) if f.endswith('.jpg')]

    def __getitem__(self, idx):
        base_id = self.ids[idx]
        imgs = []

        for m in self.modalities:
            suffix = f'_{m}' if m != 'rgb' else ''
            path = os.path.join(self.root, 'images', self.split, m, base_id + suffix + '.jpg')
            img = cv2.imread(path)
            img = cv2.resize(img, self.img_size)
            imgs.append(torch.tensor(img).permute(2, 0, 1) / 255.0)

        labels_path = os.path.join(self.root, 'labels', self.split, base_id + '.txt')
        boxes = np.loadtxt(labels_path).reshape(-1, 5)

        return torch.stack(imgs, dim=0), torch.tensor(boxes, dtype=torch.float32)

    def __len__(self):
        return len(self.ids)


import os, cv2, torch
import numpy as np
from torch.utils.data import Dataset

class MultiModalDataset(Dataset):
    def __init__(self, root, split='train', modalities=['rgb','ir'], img_size=(640,640), transforms=None):
        self.root = root
        self.split = split
        self.modalities = modalities
        self.img_size = img_size
        self.transforms = transforms

        rgb_path = os.path.join(root, 'images', split, 'rgb')
        self.ids = [os.path.splitext(f)[0] for f in os.listdir(rgb_path) if f.endswith('D:\Master\Rewa\3rd semester\Machine Learning in Robotics\Projects\Meta-Transformer-RGB-Depth-Meta-Detection\MultiModal-MetaDetector\data\images\test\ir\rgbir.jpg')]

    def __getitem__(self, idx):
        base_id = self.ids[idx]
        imgs = []

        for m in self.modalities:
            suffix = f'_{m}' if m != 'rgb' else ''
            path = os.path.join(self.root, 'images', self.split, m, base_id + suffix + '.jpg')
            img = cv2.imread(path)
            img = cv2.resize(img, self.img_size)
            imgs.append(torch.tensor(img).permute(2, 0, 1) / 255.0)

        labels_path = os.path.join(self.root, 'labels', self.split, base_id + '.txt')
        boxes = np.loadtxt(labels_path).reshape(-1, 5)

        return torch.stack(imgs, dim=0), torch.tensor(boxes, dtype=torch.float32)

    def __len__(self):
        return len(self.ids)

import torch
import torch.nn as nn

class CrossModalFusion(nn.Module):
    def __init__(self, num_modalities):
        super().__init__()
        self.fc = nn.Linear(num_modalities, 1)

    def forward(self, features):
        stacked = torch.stack(features, dim=0)  # [modalities, batch, C, H, W]
        fused = torch.mean(stacked, dim=0)      # average fusion
        return fused

import torch.nn as nn

class MetaTransformerBackbone(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU()
        )

    def forward(self, x):
        return self.encoder(x)

import torch.nn as nn

class DetectionHead(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.head = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, num_classes + 5, 1)  # class scores + bbox (x,y,w,h,obj)
        )

    def forward(self, x):
        return self.head(x)

import os
import torch
from torch.utils.data import Dataset
from PIL import Image  # For image loading; install pillow if needed
import numpy as np

class MultiModalDataset(Dataset):
    def __init__(self, root='data', split='train'):
        """
        Custom PyTorch Dataset for multimodal RGB + Depth data.
        Assumes directory structure:
        - root/split/rgb/ (e.g., image_0001.jpg)
        - root/split/depth/ (e.g., depth_0001.png)
        - Annotations: For demo, uses dummy boxes; replace with real loading (e.g., from JSON or TXT files).
        """
        self.root = root
        self.split = split
        self.rgb_dir = os.path.join(root, split, 'rgb')
        self.depth_dir = os.path.join(root, split, 'depth')
        
        # List paired files (assume filenames match, e.g., image_0001.jpg and depth_0001.png)
        self.rgb_files = sorted([f for f in os.listdir(self.rgb_dir) if f.endswith(('.jpg', '.png'))])
        self.depth_files = sorted([f for f in os.listdir(self.depth_dir) if f.endswith(('.jpg', '.png'))])
        
        # For demo; in real use, load from annotation files (e.g., COCO-style JSON)
        if len(self.rgb_files) == 0:
            # Dummy data if no files
            self.data = [(None, None)] * 10
        else:
            assert len(self.rgb_files) == len(self.depth_files), "RGB and Depth files must match in number and order."
            self.data = list(zip(self.rgb_files, self.depth_files))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        rgb_path, depth_path = self.data[idx]
        
        if rgb_path is None:
            # Dummy tensors for testing
            rgb_img = torch.rand(3, 224, 224)  # C x H x W (RGB)
            depth_img = torch.rand(1, 224, 224)  # C x H x W (Depth as single channel)
            boxes = torch.rand(5, 4)  # Dummy N x 4 boxes (x1, y1, x2, y2)
        else:
            # Load real images (normalize/transform as needed)
            rgb_img = np.array(Image.open(os.path.join(self.rgb_dir, rgb_path)).convert('RGB')) / 255.0
            rgb_img = torch.from_numpy(rgb_img.transpose(2, 0, 1)).float()  # To C x H x W
            
            depth_img = np.array(Image.open(os.path.join(self.depth_dir, depth_path)).convert('L')) / 255.0  # Grayscale
            depth_img = torch.from_numpy(depth_img[None, :, :]).float()  # To 1 x H x W
            
            # Load real boxes (dummy here; implement parsing from annotations)
            boxes = torch.rand(5, 4)  # Replace with actual loading, e.g., from TXT or JSON
        
        imgs = {'rgb': rgb_img, 'depth': depth_img}
        return imgs, boxes

import torch
import torch.nn as nn
import torch.nn.functional as F

class Data2Seq(nn.Module):
    def __init__(self, modality: str, dim: int = 768, patch_size: int = 16, num_samples_ratio: float = 0.25, k: int = 16):
        super().__init__()
        self.modality = modality
        self.dim = dim
        
        if modality == 'image':
            self.patch_size = patch_size
            self.proj = nn.Linear((patch_size ** 2) * 3, dim)  # Assume RGB; adjust channels if needed
        elif modality == 'point-cloud':
            self.num_samples_ratio = num_samples_ratio
            self.k = k
            self.proj1 = nn.Linear(3, dim // 2)  # Project 3D coords (features=0 for simplicity)
            self.proj2 = nn.Linear(dim // 2, dim)  # Final projection
        else:
            raise ValueError(f"Unsupported modality: {modality}")

    def forward(self, x):
        if self.modality == 'image':
            return self._tokenize_image(x)
        elif self.modality == 'point-cloud':
            return self._tokenize_point_cloud(x)
        else:
            raise ValueError(f"Unsupported modality: {self.modality}")

    def _tokenize_image(self, img: torch.Tensor):
        # img: [B, C, H, W] or [C, H, W] if batch=1
        if img.dim() == 3:
            img = img.unsqueeze(0)  # Add batch dim
        B, C, H, W = img.shape
        p = self.patch_size
        
        # Unfold to patches
        patches = img.unfold(2, p, p).unfold(3, p, p)  # [B, C, H/p, W/p, p, p]
        patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()  # [B, H/p, W/p, C, p, p]
        patches = patches.view(B, -1, C * p * p)  # [B, N_patches, flatten_dim]
        
        # Project
        tokens = self.proj(patches)  # [B, N_patches, dim]
        return tokens.squeeze(0) if B == 1 else tokens

    def _tokenize_point_cloud(self, pc: torch.Tensor):
        # pc: [B, N, 3] or [N, 3] if batch=1 (points only; no extra features)
        if pc.dim() == 2:
            pc = pc.unsqueeze(0)  # Add batch dim
        B, N, _ = pc.shape
        
        # Farthest Point Sampling (FPS)
        num_samples = int(N * self.num_samples_ratio)
        sampled_idx = self._farthest_point_sample(pc, num_samples)
        
        # K-Nearest Neighbors (KNN) grouping
        grouped_pc = self._knn_group(pc, sampled_idx, self.k)
        
        # Simple projection (mean pool groups for local features)
        local_feats = grouped_pc.mean(dim=-1)  # [B, num_samples, 3]
        embeds = F.relu(self.proj1(local_feats))  # [B, num_samples, dim/2]
        tokens = self.proj2(embeds)  # [B, num_samples, dim]
        
        return tokens.squeeze(0) if B == 1 else tokens

    @staticmethod
    def _farthest_point_sample(points: torch.Tensor, n_samples: int):
        """Farthest Point Sampling implementation."""
        B, N, _ = points.shape
        centroids = torch.zeros(B, n_samples, dtype=torch.long, device=points.device)
        distance = torch.ones(B, N, device=points.device) * 1e10
        farthest = torch.randint(0, N, (B,), dtype=torch.long, device=points.device)
        
        for i in range(n_samples):
            centroids[:, i] = farthest
            centroid = points[torch.arange(B).unsqueeze(1), farthest.unsqueeze(1), :].view(B, 1, 3)
            dist = torch.sum((points - centroid) ** 2, dim=-1)
            mask = dist < distance
            distance[mask] = dist[mask]
            farthest = torch.max(distance, dim=-1)[1]
        
        return centroids

    @staticmethod
    def _knn_group(points: torch.Tensor, idx: torch.Tensor, k: int):
        """Group points using KNN."""
        B, num_samples = idx.shape
        _, N, D = points.shape
        
        sampled_points = points[torch.arange(B).unsqueeze(1).repeat(1, num_samples), idx, :]  # [B, num_samples, D]
        
        diff = points.unsqueeze(1) - sampled_points.unsqueeze(2)  # [B, num_samples, N, D]
        dist = torch.sum(diff ** 2, dim=-1)  # [B, num_samples, N]
        
        _, knn_idx = torch.topk(dist, k, dim=-1, largest=False, sorted=False)  # [B, num_samples, k]
        
        grouped = points[torch.arange(B).unsqueeze(1).unsqueeze(2).repeat(1, num_samples, k), knn_idx, :]  # [B, num_samples, k, D]
        
        return grouped

# Optional helper: Convert depth map to point cloud (if your depth is 2D image)
def depth_to_point_cloud(depth_map: torch.Tensor, fx=525.0, fy=525.0, cx=319.5, cy=239.5):
    """Project depth map [1, H, W] to point cloud [H*W, 3]. Assume camera intrinsics."""
    if depth_map.dim() == 3:
        depth_map = depth_map.squeeze(0)  # [H, W]
    H, W = depth_map.shape
    i, j = torch.meshgrid(torch.arange(H, device=depth_map.device), torch.arange(W, device=depth_map.device), indexing='ij')
    x = (j - cx) * depth_map / fx
    y = (i - cy) * depth_map / fy
    z = depth_map
    pc = torch.stack([x, y, z], dim=-1).view(-1, 3)  # [H*W, 3]
    pc = pc[pc[:, 2] > 0]  # Filter invalid depths
    return pc
